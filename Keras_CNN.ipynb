{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras-CNN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "xi-5alxh1tEK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Using Keras and CNN"
      ]
    },
    {
      "metadata": {
        "id": "T6rCtDGk1tEL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zT6lCZrK1tEO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loading up the raw data set"
      ]
    },
    {
      "metadata": {
        "id": "U-CE5WzE1tEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "208fc433-dd2d-4720-ddb5-d95da25eeb03"
      },
      "cell_type": "code",
      "source": [
        "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UR_G8DRb1tEU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since we're treating the data as 2D images of 28x28 pixels instead of a flattened stream of 784 pixels, we need to shape it accordingly. Depending on the data format Keras is set up for, this may be 1x28x28 or 28x28x1 (the \"1\" indicates a single color channel, as this is just grayscale. If we were dealing with color images, it would be 3 instead of 1 since we'd have red, green, and blue color channels)"
      ]
    },
    {
      "metadata": {
        "id": "f01BsBZ91tEW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 1, 28, 28)\n",
        "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 1, 28, 28)\n",
        "    input_shape = (1, 28, 28)\n",
        "else:\n",
        "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 28, 28, 1)\n",
        "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 28, 28, 1)\n",
        "    input_shape = (28, 28, 1)\n",
        "    \n",
        "train_images = train_images.astype('float32')\n",
        "test_images = test_images.astype('float32')\n",
        "train_images /= 255\n",
        "test_images /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UPmqAovg1tEZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Converting the train and test labels to be categorical in one-hot format:"
      ]
    },
    {
      "metadata": {
        "id": "BOjjwSvw1tEb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_labels = tensorflow.keras.utils.to_categorical(mnist_train_labels, 10)\n",
        "test_labels = tensorflow.keras.utils.to_categorical(mnist_test_labels, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d1sR1U4V1tEg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Printing out one of the training images with its label:"
      ]
    },
    {
      "metadata": {
        "id": "lxZpyWA11tEi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "474136c8-36d0-4015-91f6-07d524bceeef"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_sample(num):\n",
        "    #Print the one-hot array of this sample's label \n",
        "    print(train_labels[num])  \n",
        "    #Print the label converted back to a number\n",
        "    label = train_labels[num].argmax(axis=0)\n",
        "    #Reshape the 768 values to a 28x28 image\n",
        "    image = train_images[num].reshape([28,28])\n",
        "    plt.title('Sample: %d  Label: %d' % (num, label))\n",
        "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
        "    plt.show()\n",
        "    \n",
        "display_sample(1234)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFZCAYAAAARqQ0OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwhIADAhLADAgLAHAgLAEAAPCEgAM/g+y/SD3ryh4eAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "f8mpF7Pl1tEn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll start with a 2D convolution of the image - it's set up to take 32 windows, or \"filters\", of each image, each filter being 3x3 in size.\n",
        "\n",
        "We then run a second convolution on top of that with 64 3x3 windows. \n",
        "\n",
        "Next we apply a MaxPooling2D layer that takes the maximum of each 2x2 result to distill the results down into something more manageable.\n",
        "\n",
        "A dropout filter is then applied to prevent overfitting.\n",
        "\n",
        "Next we flatten the 2D layer we have at this stage into a 1D layer. So at this point we can just pretend we have a traditional multi-layer perceptron...\n",
        "\n",
        "... and feed that into a hidden, flat layer of 128 units.\n",
        "\n",
        "We then apply dropout again to further prevent overfitting.\n",
        "\n",
        "And finally, we feed that into our final 10 units where softmax is applied to choose our category of 0-9."
      ]
    },
    {
      "metadata": {
        "id": "sPyCRAXm1tEo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "da09daf2-d9b5-4312-8153-d299753eacee"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "# 64 3x3 kernels\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# Reduce by taking the max of each 2x2 block\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# Dropout to avoid overfitting\n",
        "model.add(Dropout(0.25))\n",
        "# Flatten the results to one dimension for passing into our final layer\n",
        "model.add(Flatten())\n",
        "# A hidden layer to learn with\n",
        "model.add(Dense(128, activation='relu'))\n",
        "# Another dropout\n",
        "model.add(Dropout(0.5))\n",
        "# Final categorization from 0-9 with softmax\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NLbFMvVW1tEt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "f338b7a8-f9a8-4851-95c4-c0cbcf17f6d2"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               1179776   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,199,882\n",
            "Trainable params: 1,199,882\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nRxRWCx-1tEz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since it is a  multiple categorization, so categorical_crossentropy is still the right loss function to use.I have used the Adam optimizer."
      ]
    },
    {
      "metadata": {
        "id": "ynSfjm0Q1tE1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_9UPj-iR1tE8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using batches of 32.\n"
      ]
    },
    {
      "metadata": {
        "id": "PLo_7vOQ1tE9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "outputId": "c1bc802e-5b0f-48c8-c4f3-ee5fc4b1799b"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(train_images, train_labels,\n",
        "                    batch_size=32,\n",
        "                    epochs=10,\n",
        "                    verbose=2,\n",
        "                    validation_data=(test_images, test_labels))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/10\n",
            " - 195s - loss: 0.1883 - acc: 0.9420 - val_loss: 0.0430 - val_acc: 0.9857\n",
            "Epoch 2/10\n",
            " - 193s - loss: 0.0779 - acc: 0.9774 - val_loss: 0.0325 - val_acc: 0.9899\n",
            "Epoch 3/10\n",
            " - 191s - loss: 0.0589 - acc: 0.9819 - val_loss: 0.0286 - val_acc: 0.9913\n",
            "Epoch 4/10\n",
            " - 192s - loss: 0.0481 - acc: 0.9855 - val_loss: 0.0312 - val_acc: 0.9907\n",
            "Epoch 5/10\n",
            " - 245s - loss: 0.0414 - acc: 0.9870 - val_loss: 0.0276 - val_acc: 0.9920\n",
            "Epoch 6/10\n",
            " - 407s - loss: 0.0352 - acc: 0.9886 - val_loss: 0.0284 - val_acc: 0.9915\n",
            "Epoch 7/10\n",
            " - 412s - loss: 0.0304 - acc: 0.9898 - val_loss: 0.0288 - val_acc: 0.9921\n",
            "Epoch 8/10\n",
            " - 407s - loss: 0.0268 - acc: 0.9913 - val_loss: 0.0302 - val_acc: 0.9921\n",
            "Epoch 9/10\n",
            " - 412s - loss: 0.0249 - acc: 0.9921 - val_loss: 0.0272 - val_acc: 0.9928\n",
            "Epoch 10/10\n",
            " - 406s - loss: 0.0241 - acc: 0.9924 - val_loss: 0.0303 - val_acc: 0.9924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BaORZ_a11tFE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a4004b51-5cba-4987-8207-06e03151125b"
      },
      "cell_type": "code",
      "source": [
        "score = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.030254188442353096\n",
            "Test accuracy: 0.9924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0b1Qg-pE1tFK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We obtained over 99% accuracy with just 10 epochs!"
      ]
    }
  ]
}